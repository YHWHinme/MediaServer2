{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Ollama Chat Test\n",
    "\n",
    "Testing basic chat functionality with Ollama + LangChain.\n",
    "This strips down the MediaServer2 app to just chat generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run this first if needed)\n",
    "# !pip install langchain-ollama langchain-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports - same as your main.py but simplified\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Ollama at: http://172.16.5.234:3000\n",
      "Using model: gemma3:270m\n"
     ]
    }
   ],
   "source": [
    "# Configuration - same as your main.py\n",
    "OLLAMA_BASE_URL = \"http://172.16.5.234:3000\"\n",
    "CHAT_MODEL = \"gemma3:270m\"\n",
    "\n",
    "print(f\"Connecting to Ollama at: {OLLAMA_BASE_URL}\")\n",
    "print(f\"Using model: {CHAT_MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chat client initialized successfully\n"
     ]
    }
   ],
   "source": [
    "# Initialize Chat Client - same as your Agent class\n",
    "try:\n",
    "    chat_client = ChatOllama(\n",
    "        base_url=OLLAMA_BASE_URL,\n",
    "        model=CHAT_MODEL\n",
    "    )\n",
    "    print(\"‚úÖ Chat client initialized successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to initialize chat client: {e}\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending test message: Hello! Can you tell me what 2+2 equals?\n",
      "\n",
      "ü§ñ Response: 2 + 2 = 4\n",
      "\n",
      "‚úÖ Connection test successful!\n"
     ]
    }
   ],
   "source": [
    "# Test basic connection\n",
    "try:\n",
    "    # Simple test message\n",
    "    test_message = \"Hello! Can you tell me what 2+2 equals?\"\n",
    "    print(f\"Sending test message: {test_message}\")\n",
    "    \n",
    "    response = chat_client.invoke(test_message)\n",
    "    print(f\"\\nü§ñ Response: {response.content}\")\n",
    "    print(\"‚úÖ Connection test successful!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Connection test failed: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"1. Check if Ollama server is running\")\n",
    "    print(\"2. Verify the model is available: ollama list\")\n",
    "    print(\"3. Test direct connection: curl http://172.16.5.60:3000/api/tags\")\n",
    "    print(f\"4. Current error: {type(e).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive chat function\n",
    "def chat_with_ai():\n",
    "    print(\"\\nü§ñ Starting interactive chat session\")\n",
    "    print(\"Type 'quit' to exit\\n\")\n",
    "    \n",
    "    conversation_history = []\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            user_input = input(\"You: \").strip()\n",
    "            \n",
    "            if user_input.lower() in ['quit', 'exit', 'bye']:\n",
    "                print(\"\\nüëã Goodbye!\")\n",
    "                break\n",
    "            \n",
    "            if not user_input:\n",
    "                continue\n",
    "            \n",
    "            # Add user message to history\n",
    "            conversation_history.append(HumanMessage(content=user_input))\n",
    "            \n",
    "            # Get AI response\n",
    "            print(\"ü§ñ Thinking...\")\n",
    "            response = chat_client.invoke(conversation_history)\n",
    "            \n",
    "            # Print response\n",
    "            print(f\"ü§ñ AI: {response.content}\\n\")\n",
    "            \n",
    "            # Add AI response to history\n",
    "            conversation_history.append(AIMessage(content=response.content))\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\nüëã Chat interrupted. Goodbye!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error during chat: {e}\")\n",
    "            print(\"Continuing...\\n\")\n",
    "\n",
    "# Uncomment to start interactive chat\n",
    "# chat_with_ai()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with different prompts\n",
    "test_prompts = [\n",
    "    \"Explain quantum computing in simple terms\",\n",
    "    \"Write a haiku about programming\",\n",
    "    \"What are the benefits of using Python for AI?\",\n",
    "    \"Tell me a joke about computers\"\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing with various prompts:\\n\")\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    try:\n",
    "        print(f\"{i}. Prompt: {prompt}\")\n",
    "        response = chat_client.invoke(prompt)\n",
    "        print(f\"   Response: {response.content[:100]}...\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {e}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model information and diagnostics\n",
    "try:\n",
    "    print(\"üîç Model Information:\")\n",
    "    print(f\"   Model: {CHAT_MODEL}\")\n",
    "    print(f\"   Base URL: {OLLAMA_BASE_URL}\")\n",
    "    \n",
    "    # Test model capabilities\n",
    "    test_response = chat_client.invoke(\"Say 'Connection successful' if you can read this.\")\n",
    "    print(f\"   Status: ‚úÖ {test_response.content.strip()}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   Status: ‚ùå Connection failed - {e}\")\n",
    "    \n",
    "print(\"\\nüìä Diagnostics complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
